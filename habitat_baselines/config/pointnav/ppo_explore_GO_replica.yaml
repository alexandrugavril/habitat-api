BASE_TASK_CONFIG_PATH: "configs/tasks/explore_replica.yaml"
TRAINER_NAME: "ppoAimasReachability"
ENV_NAME: "NavRLExploration"
SIMULATOR_GPU_ID: {gpu_id}
TORCH_GPU_ID: {gpu_id}
VIDEO_OPTION: ["disk", "tensorboard"]
TENSORBOARD_DIR: "results/{results_prefix}/explore_GO_test/tb"
VIDEO_DIR: "results/{results_prefix}/explore_GO_test/video_dir"
EVAL_CKPT_PATH_DIR: "results/{results_prefix}/explore_GO_test/checkpoints"
CHECKPOINT_FOLDER: "results/{results_prefix}/explore_GO_test/checkpoints"
LOG_FILE: "results/{results_prefix}/explore_GO_test/log_file"
TEST_EPISODE_COUNT: 2
NUM_PROCESSES: 2  # Lets move to 12 number of envs
SENSORS: ["RGB_SENSOR", "DEPTH_SENSOR"]
NUM_UPDATES: 5000 # So as to train on 15 Mil frames
LOG_INTERVAL: 10
CHECKPOINT_INTERVAL: 50
COMMIT: "{commit_hash}"
TRANSFORM_RGB:
  ENABLED: True
TRANSFORM_DEPTH:
  ENABLED: True
  std_noise: 0.2
  depth_min_th: 0.04
  depth_max_th: 0.8
  noise_interval: 0.03

TASK_CONFIG:
  TASK:
    SENSORS: ["SET_GOAL", 'POINTGOAL_WITH_GPS_COMPASS_SENSOR']
    WITH_TARGET_ENCODING: True
    POSSIBLE_ACTIONS: ['NOISY_MOVE_FORWARD', 'NOISY_TURN_LEFT', 'NOISY_TURN_RIGHT']
  SIMULATOR:
    HABITAT_SIM_V0:
      GPU_DEVICE_ID: {gpu_id}
  ENVIRONMENT:
    ITERATOR_OPTIONS:
      GROUP_BY_SCENE: False
      SHUFFLE: True

RL:
  SLACK_REWARD: -0.01
  COLLISION_REWARD_ENABLED: True
  SUCCESS_REWARD: 1.
  COLLISION_REWARD: -0.1
  COLLISION_DISTANCE: 0.3
  PPO:
    visual_encoder: SimpleCNN
    visual_encoder_dropout: 0.0
    channel_scale: 1
    # ppo params
    clip_param: 0.1 # Episodic curiosity has some kind of decaying cliprange
    ppo_epoch: 4
    num_mini_batch: 4 # They have 4? which is slower should be increase no_envs? which give  3/envs/batch
    value_loss_coef: 0.5
    entropy_coef: 0.01  # They use much smaller entropy coef 0.0066 (with grid oracle)
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.5
    num_steps: 256  # Episodic curiosity 256
    hidden_size: 512
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: True
    reward_window_size: 50

  REACHABILITY:
    enabled: False
    train: False
    skip_train_ppo_without_rtrain: False
    only_intrinsic_reward: False
    experience_buffer_size: 719992
    num_recurrent_steps: 1
    batch_size: 64
    num_train_epochs: 10
    feature_extractor_size: 512
    memory_size: 200
    log_freq: 100
    grid_resolution: 1
    optimizer: "Adam"
    optimizer_args:
      lr: 0.00025
    max_action_distance_k: 5
    negative_sample_multiplier: 5
    curiosity_bonus_scale_a: 0.030
    reward_shift_b: 0.5
    novelty_threshold: 0.5
    similarity_aggregation: "percentile"
    similarity_percentile: 90
