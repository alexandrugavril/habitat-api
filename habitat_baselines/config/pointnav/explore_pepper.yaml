BASE_TASK_CONFIG_PATH: "configs/tasks/explore_pepper.yaml"
TRAINER_NAME: "ppoAimasReachability"
ENV_NAME: "NavRLExploration"
SIMULATOR_GPU_ID: {gpu_id}
TORCH_GPU_ID: {gpu_id}
VIDEO_OPTION: ["disk", "tensorboard"]
TENSORBOARD_DIR: "results/{results_prefix}/explore_GO_test/tb"
VIDEO_DIR: "results/{results_prefix}/explore_GO_test/video_dir"
EVAL_CKPT_PATH_DIR: "results/{results_prefix}/explore_GO_test/checkpoints"
CHECKPOINT_FOLDER: "results/{results_prefix}/explore_GO_test/checkpoints"
LOG_FILE: "results/{results_prefix}/explore_GO_test/log_file"
TEST_EPISODE_COUNT: 2
NUM_PROCESSES: 12  # Lets move to 12 number of envs
SENSORS: ["RGB_SENSOR", "DEPTH_SENSOR"]
NUM_UPDATES: 5000 # So as to train on 15 Mil frames
LOG_INTERVAL: 10
CHECKPOINT_INTERVAL: 50
COMMIT: "{commit_hash}"

RL:
  COLLISION_REWARD_ENABLED: True
  PPO:
    # ppo params
    clip_param: 0.1 # Episodic curiosity has some kind of decaying cliprange
    ppo_epoch: 4
    num_mini_batch: 4 # They have 4? which is slower should be increase no_envs? which give  3/envs/batch
    value_loss_coef: 0.5
    entropy_coef: 0.01  # They use much smaller entropy coef 0.0066 (with grid oracle)
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.5
    num_steps: 256  # Episodic curiosity 256
    hidden_size: 512
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: True
    reward_window_size: 50

  REACHABILITY:
    train: False
    skip_train_ppo_without_rtrain: False
    only_intrinsic_reward: False
    experience_buffer_size: 719992
    num_recurrent_steps: 1
    batch_size: 64
    num_train_epochs: 10
    feature_extractor_size: 512
    memory_size: 200
    log_freq: 100
    grid_resolution: 1
    optimizer: "Adam"
    optimizer_args:
      lr: 0.00025
    max_action_distance_k: 5
    negative_sample_multiplier: 5
    curiosity_bonus_scale_a: 0.030
    reward_shift_b: 0.5
    novelty_threshold: 0.5
    similarity_aggregation: "percentile"
    similarity_percentile: 90
